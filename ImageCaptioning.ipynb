{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qToVeHMo8onr",
        "colab_type": "text"
      },
      "source": [
        "**Implementing image captioning model using flicker_8k dataset. 7k images for training and 1k images for test purpose. I am using VGG16 model as the part of the transfer learning.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_yG0cr78xwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img # used to import image in PIL format\n",
        "from keras.preprocessing.image import img_to_array # used to convert the PIL image to numpy array format\n",
        "from keras.applications.vgg16 import preprocess_input #used to extract features from the image\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o00o_P9W_ZpM",
        "colab_type": "code",
        "outputId": "736b5e8d-050f-4bc7-ca91-ea742c960fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def extract_features(directory):\n",
        "  #load the model\n",
        "  model = VGG16()\n",
        "  # re-structure the model\n",
        "  model.layers.pop()\n",
        "  model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "  #summarize\n",
        "  # print(model.summary())\n",
        "\n",
        "  #extract features from images\n",
        "  features = dict()\n",
        "  for name in listdir(directory):\n",
        "    #load image from file\n",
        "    filepath = directory + '/' + name\n",
        "    image = load_img(filepath, target_size=(224, 224))\n",
        "    #convert the image to numpy array\n",
        "    image = img_to_array(image)\n",
        "    #reshape the image\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    #prepare the image for VGG model\n",
        "    image = preprocess_input(image)\n",
        "    #get features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    image_id = name.split('.')[0]\n",
        "    #sore the image\n",
        "    features[image_id] = feature\n",
        "\n",
        "  return features\n",
        "\n",
        "# extract features from all th images\n",
        "directory = 'flicker-dataset'\n",
        "features = extract_features(directory)\n",
        "print(len(features))\n",
        "# dump(features, open('Image_Caption_Modelling/features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_R2wpwWO-0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "  #open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  #read all text\n",
        "  text = file.read()\n",
        "  #close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "#extract descriptions from images\n",
        "def load_descriptions(doc):\n",
        "  mapping = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "    if len(line)< 0:\n",
        "      continue\n",
        "    tokens = line.split()\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    image_id = image_id.split('.')[0]\n",
        "    image_desc = 'starteq '+\" \".join(image_desc)+' endeq'\n",
        "\n",
        "    if image_id not in mapping:\n",
        "      mapping[image_id] = list()\n",
        "    \n",
        "    #store description\n",
        "    mapping[image_id].append(image_desc)\n",
        "\n",
        "  return mapping\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzyEvjAuQQjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_vocabulary(descriptions):\n",
        "  #build list of all description strings\n",
        "  all_desc = set()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "  return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c9SYHdFYylc",
        "colab_type": "code",
        "outputId": "78248b61-011c-4605-e67e-affdab4925de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "filename = '/content/Flickr8k.token.txt'\n",
        "\n",
        "#load descriptions\n",
        "doc = load_doc(filename)\n",
        "\n",
        "#parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print(descriptions)\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print(len(vocabulary))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'10815824_2997e03d76': ['starteq A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel . endeq', 'starteq A girl and her horse stand by a fire . endeq', \"starteq A girl holding a horse 's lead behind a fire . endeq\", 'starteq A man , and girl and two horses are near a contained fire . endeq', 'starteq Two people and two horses watching a fire . endeq']}\n",
            "33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4B6Pg2fGthS",
        "colab_type": "code",
        "outputId": "8b19141e-ee17-49c5-e7a3-93bced8738e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "l = list()\n",
        "\n",
        "for key in descriptions.keys():\n",
        "    [l.append(d) for d in descriptions[key]]\n",
        "\n",
        "l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['starteq A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel . endeq',\n",
              " 'starteq A girl and her horse stand by a fire . endeq',\n",
              " \"starteq A girl holding a horse 's lead behind a fire . endeq\",\n",
              " 'starteq A man , and girl and two horses are near a contained fire . endeq',\n",
              " 'starteq Two people and two horses watching a fire . endeq']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Yva_AIcgDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the dictionary of clean descriptions to the list of descriptions\n",
        "def to_lines(descriptions):\n",
        "  all_desc = list()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbuJkEZFEhdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fit the tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "def create_tokenizer(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB-xesToFGRK",
        "colab_type": "code",
        "outputId": "34437bf2-aeed-42bc-8523-cc3f8e098683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(to_lines(descriptions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['starteq A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel . endeq', 'starteq A girl and her horse stand by a fire . endeq', \"starteq A girl holding a horse 's lead behind a fire . endeq\", 'starteq A man , and girl and two horses are near a contained fire . endeq', 'starteq Two people and two horses watching a fire . endeq']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaMLCmIQFtaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate the length of the descriptions with most words\n",
        "def max_length(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  return max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaYw--zlHcmQ",
        "colab_type": "code",
        "outputId": "e46340ef-8e85-4c54-bd2b-4c0c652e30d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "max_length(descriptions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4nokA1QHeOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from numpy import array\n",
        "\n",
        "tokenizer = create_tokenizer(descriptions)\n",
        "#create sequences is written to generate the data on which model will train itself. \n",
        "#This is mainly done when we have less memory\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each description for the image\n",
        "\tfor desc in desc_list:\n",
        "\t\t# encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t# split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t# encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# store\n",
        "\t\t\tX1.append(photo)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlECNWFJkNnY",
        "colab_type": "code",
        "outputId": "5805edb7-9056-4052-fa65-7f5e66eb39e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "c = create_sequences(tokenizer, max_length(descriptions), to_lines(descriptions), features[\"10815824_2997e03d76\"])\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32), array([[ 0,  0,  0, ...,  0,  0,  2],\n",
            "       [ 0,  0,  0, ...,  0,  2,  1],\n",
            "       [ 0,  0,  0, ...,  2,  1,  9],\n",
            "       ...,\n",
            "       [ 0,  0,  0, ...,  8, 12, 29],\n",
            "       [ 0,  0,  0, ..., 12, 29,  1],\n",
            "       [ 0,  0,  0, ..., 29,  1,  4]], dtype=int32), array([[0., 1., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 1., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV_Kxl6EOMDg",
        "colab_type": "code",
        "outputId": "e6a77b87-5421-4fa9-c36f-1a1b3c2e24f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "tokenizer = create_tokenizer(descriptions)\n",
        "desc_list = to_lines(descriptions)\n",
        "print(desc_list[0])\n",
        "l = tokenizer.texts_to_sequences([desc_list[0]])[0]\n",
        "print(max_length)\n",
        "in_seq, out_seq = l[:1], l[1]\n",
        "print(in_seq)\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "vocab_size = len(vocabulary)\n",
        "in_seq = pad_sequences([in_seq], maxlen=max_length(descriptions))[0]\n",
        "out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "print(in_seq)\n",
        "print(out_seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starteq A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel . endeq\n",
            "<function max_length at 0x7fd3c5133b70>\n",
            "[2]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td2kJXZ3OPQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjbNFExqUGo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(vocab_size, max_length):\n",
        "  # featue extract model\n",
        "  inputs1 = Input(shape=(4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "  #sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "\n",
        "  #decorder model\n",
        "  decorder1 = add([fe2, se3])\n",
        "  decorder2 = Dense(256, activation=\"relu\")(decorder1)\n",
        "  outputs = Dense(vocab_size, activation=\"softmax\")(decorder2)\n",
        "\n",
        "  #tie the all\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "  #model summary\n",
        "  print(model.summary)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ju3vpjywpjd",
        "colab_type": "code",
        "outputId": "17fec09a-7e7c-4f85-8f83-a1dac6aa545f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "descriptions, features[\"10815824_2997e03d76\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'10815824_2997e03d76': ['starteq A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel . endeq',\n",
              "   'starteq A girl and her horse stand by a fire . endeq',\n",
              "   \"starteq A girl holding a horse 's lead behind a fire . endeq\",\n",
              "   'starteq A man , and girl and two horses are near a contained fire . endeq',\n",
              "   'starteq Two people and two horses watching a fire . endeq']},\n",
              " array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGpCK3hC27y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(descriptions, photos, tokenizer, max_length):\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
        "\t\t\tyield [[in_img, in_seq], out_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KufjSbLZtijC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "# print(vocab_size)\n",
        "# print(max_length(descriptions))\n",
        "# model = model(vocab_size, max_length(descriptions))\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=\"adam\",\n",
        "#               metrics=['accuracy'])\n",
        "# model.fit(features, descriptions, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFTBHp2C3AI9",
        "colab_type": "code",
        "outputId": "fb9f965b-a70c-4478-cd8a-4673d38cc344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "source": [
        "# train the model\n",
        "model = model(vocab_size, max_length(descriptions))\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 20\n",
        "steps = len(descriptions)\n",
        "for i in range(epochs):\n",
        "\t# create the data generator\n",
        "\tgenerator = data_generator(descriptions, features, tokenizer, max_length(descriptions))\n",
        "\t# fit for one epoch\n",
        "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\t# save model\n",
        "\tmodel.save('model_' + str(i) + '.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Network.summary of <keras.engine.training.Model object at 0x7fd3c4f244a8>>\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 5s 5s/step - loss: 5.4888\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 5.2994\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 3.9639\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 4.4820\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 3.8802\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 3.6406\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 3.5165\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 3.7035\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 3.4114\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 3.3155\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 3.3482\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 3.1843\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 3.2727\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 3.1611\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 3.2683\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 3.2490\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 3.3484\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 3.2978\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 3.1190\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 3.2817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPQ-pfRgqTmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_for_id(number, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == number:\n",
        "      return word\n",
        "  return none"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrS0JsMisA6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate description for image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "  in_text = 'starteq'\n",
        "  for i in range(max_length):\n",
        "    #integer encode input sequence\n",
        "    sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    #pad input\n",
        "    sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "    #predict next word\n",
        "    yhat = model.predict([photo, sequence], verbose=0)\n",
        "    print(yhat)\n",
        "    #convert probability to integer\n",
        "    yhat = argmax(yhat)\n",
        "    #map integer to word\n",
        "    word = word_for_id(yhat, tokenizer)\n",
        "    if word is None:\n",
        "      break\n",
        "    in_text += ' ' + word\n",
        "    if word == 'endseq':\n",
        "      break\n",
        "\n",
        "  return in_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTFp1slLtXYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('model_18.h5')\n",
        "# load and prepare the photograph\n",
        "photo = extract_features('/content/test')\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, max_length(descriptions))\n",
        "\n",
        "print(description)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0qgKa849_4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}